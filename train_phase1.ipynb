{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cd2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchaudio scikit-learn numpy\n",
    "#!pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a0a21f",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b418dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.preprocessing.dataset import (\n",
    "    list_ravdess_files,\n",
    "    filter_audio_speech,\n",
    "    RavdessDataset,\n",
    "    extract_label_idx,\n",
    "    IDX2LABEL,\n",
    ")\n",
    "from src.models.crnn import CRNN\n",
    "from src.utils import set_seed, evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from src.visual_analysis.visual_analysis import *\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e13597",
   "metadata": {},
   "source": [
    "# DEFINIZIONE DATA BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59582a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "set_seed(42)\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "N_FOLDS = 6\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# 1) lista file\n",
    "all_files = list_ravdess_files(DATA_ROOT)\n",
    "all_files = filter_audio_speech(all_files)\n",
    "if len(all_files) == 0:\n",
    "    raise RuntimeError(\"Nessun file trovato. Controlla DATA_ROOT e filter_audio_speech().\")\n",
    "\n",
    "# 2) labels per stratify\n",
    "labels = [extract_label_idx(fp) for fp in all_files]\n",
    "print(\"Tot files:\", len(all_files))\n",
    "print(\"Distribuzione classi:\", Counter(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4afc0",
   "metadata": {},
   "source": [
    "# SPLIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19a8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serve per vedere se stai usando la CPU o se hai CUDA\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# IMPORTANTE\n",
    "# se qui la prima riga ti ritorna True allora sotto quando scrivi i DataLoader metti i pinmemory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b124db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.dataset import parse_ravdess_filename\n",
    "\n",
    "def split_by_speakers(filepaths, train_speakers, val_speakers, test_speakers):\n",
    "    train, val, test = [], [], []\n",
    "    for fp in filepaths:\n",
    "        actor = parse_ravdess_filename(fp)[\"actor\"]\n",
    "        if actor in train_speakers:\n",
    "            train.append(fp)\n",
    "        elif actor in val_speakers:\n",
    "            val.append(fp)\n",
    "        elif actor in test_speakers:\n",
    "            test.append(fp)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# Funzione per Mixup\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf96ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "actors = [parse_ravdess_filename(fp)[\"actor\"] for fp in all_files]\n",
    "\n",
    "print(f\"Tot files: {len(all_files)}\")\n",
    "print(f\"Distribuzione classi: {Counter(labels)}\")\n",
    "print(f\"Numero attori unici: {len(set(actors))}\")\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "\n",
    "# Lista per salvare i risultati di ogni fold\n",
    "fold_results = []\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CROSS-VALIDATION CON {N_FOLDS} FOLDS (SPEAKER-INDEPENDENT)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for fold_idx, (train_val_idx, test_idx) in enumerate(gkf.split(all_files, labels, groups=actors)):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_idx + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # A) SPLIT DEI FILE\n",
    "    # =====================================================\n",
    "    \n",
    "    # Test set per questo fold (speaker completamente separati)\n",
    "    test_files = [all_files[i] for i in test_idx]\n",
    "    train_val_files = [all_files[i] for i in train_val_idx]\n",
    "    \n",
    "    # Estrai info per train_val\n",
    "    train_val_actors = [actors[i] for i in train_val_idx]\n",
    "    train_val_labels = [labels[i] for i in train_val_idx]\n",
    "    \n",
    "    # Test actors\n",
    "    test_actors = sorted(set([actors[i] for i in test_idx]))\n",
    "    \n",
    "    # =====================================================\n",
    "    # B) ULTERIORE SPLIT TRAIN/VAL (speaker-independent anche qui)\n",
    "    # =====================================================\n",
    "    \n",
    "    unique_train_val_actors = list(set(train_val_actors))\n",
    "    val_size = max(2, len(unique_train_val_actors) // 6)  # ~15-17% per validation\n",
    "    \n",
    "    # Bilanciamento per genere (RAVDESS: dispari=M, pari=F)\n",
    "    male_actors_tv = [a for a in unique_train_val_actors if int(a) % 2 == 1]\n",
    "    female_actors_tv = [a for a in unique_train_val_actors if int(a) % 2 == 0]\n",
    "    \n",
    "    # Scegli attori per validation bilanciati per genere\n",
    "    import random\n",
    "    random.seed(42 + fold_idx)  # Seed diverso per ogni fold\n",
    "    \n",
    "    val_actors = (\n",
    "        random.sample(male_actors_tv, min(val_size // 2, len(male_actors_tv))) +\n",
    "        random.sample(female_actors_tv, min(val_size // 2, len(female_actors_tv)))\n",
    "    )\n",
    "    train_actors = [a for a in unique_train_val_actors if a not in val_actors]\n",
    "    \n",
    "    # Dividi i file\n",
    "    train_files = [f for f, a in zip(train_val_files, train_val_actors) if a in train_actors]\n",
    "    val_files = [f for f, a in zip(train_val_files, train_val_actors) if a in val_actors]\n",
    "    \n",
    "    print(f\"Train: {len(train_files)} files, {len(train_actors)} speakers {sorted(train_actors)}\")\n",
    "    print(f\"Val:   {len(val_files)} files, {len(val_actors)} speakers {sorted(val_actors)}\")\n",
    "    print(f\"Test:  {len(test_files)} files, {len(test_actors)} speakers {test_actors}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # C) CLASS WEIGHTS\n",
    "    # =====================================================\n",
    "    \n",
    "    train_labels_fold = [extract_label_idx(fp) for fp in train_files]\n",
    "    counts = Counter(train_labels_fold)\n",
    "    \n",
    "    weights = torch.tensor([1.0 / counts[i] for i in range(8)], dtype=torch.float, device=device)\n",
    "    weights = weights / weights.sum() * 8\n",
    "    \n",
    "    print(f\"Class counts: {counts}\")\n",
    "    print(f\"Weights: {weights.detach().cpu().numpy().round(3)}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # D) DATASET E DATALOADER\n",
    "    # =====================================================\n",
    "    \n",
    "    AUG_ON = True\n",
    "    \n",
    "    aug_cfg = {\n",
    "        \"gain\": True,          # Abilitato\n",
    "        \"gain_db\": (-3, 3),    # Range più ampio\n",
    "        \n",
    "        \"time_shift\": True,    \n",
    "        \"time_shift_s\": 0.05,  # Più generoso\n",
    "        \n",
    "        \"noise\": True,\n",
    "        \"snr_db\": (25, 40),    # SNR più realistico\n",
    "        \n",
    "        \"speed\": True,\n",
    "        \"speed_range\": (0.9, 1.1),\n",
    "        \"vtlp\": False,\n",
    "\n",
    "        \"reverb\": False,        # Abilitato\n",
    "        \"reverb_ir_s\": 0.08,\n",
    "        \"reverb_decay\": 0.3,\n",
    "    }\n",
    "    \n",
    "    train_ds = RavdessDataset(train_files, augmentation=AUG_ON, aug_config=aug_cfg)\n",
    "    val_ds   = RavdessDataset(val_files,   augmentation=False)\n",
    "    test_ds  = RavdessDataset(test_files,  augmentation=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              num_workers=0, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=0, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # =====================================================\n",
    "    # E) MODEL, OPTIMIZER, SCHEDULER\n",
    "    # =====================================================\n",
    "    \n",
    "    model = CRNN(n_classes=8, n_mels=64).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.5, patience=6\n",
    "    )\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_path = f\"best_fold_{fold_idx}.pt\"  # Path diverso per ogni fold\n",
    "    \n",
    "    # History per questo fold\n",
    "    val_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "\n",
    "    print(f\"\\nInizio training fold {fold_idx + 1}...\")\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x, y, _ in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits, y, weight=weights, label_smoothing=0.1\n",
    "            )\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * y.size(0)\n",
    "            running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = running_correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "        test_loss_epoch, test_acc_epoch = evaluate(model, test_loader, device)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        val_acc_hist.append(val_acc)\n",
    "        test_acc_hist.append(test_acc_epoch)\n",
    "        \n",
    "        # Print ogni 5 epoche\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "                f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "                f\"val loss {val_loss:.4f} acc {val_acc:.4f} | \"\n",
    "                f\"test acc {test_acc_epoch:.4f}\"\n",
    "            )\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "    # =====================================================\n",
    "    # G) VALUTAZIONE FINALE DEL FOLD\n",
    "    # =====================================================\n",
    "    \n",
    "    # Carica il miglior modello\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    \n",
    "    final_test_loss, final_test_acc = evaluate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RISULTATI FOLD {fold_idx + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Final Test Accuracy:      {final_test_acc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x).cpu()\n",
    "            preds = logits.argmax(dim=1).numpy().tolist()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(y.numpy().tolist())\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    target_names = [IDX2LABEL[i] for i in range(8)]\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n",
    "    \n",
    "    # =====================================================\n",
    "    # H) SALVA RISULTATI DEL FOLD\n",
    "    # =====================================================\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold_idx,\n",
    "        'test_speakers': test_actors,\n",
    "        'val_speakers': sorted(val_actors),\n",
    "        'train_speakers': sorted(train_actors),\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'final_test_acc': final_test_acc,\n",
    "        'val_acc_history': val_acc_hist,\n",
    "        'test_acc_history': test_acc_hist,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00710061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62b64e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"RISULTATI FINALI CROSS-VALIDATION ({N_FOLDS} FOLDS)\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Estrai metriche\n",
    "test_accs = [r['final_test_acc'] for r in fold_results]\n",
    "val_accs = [r['best_val_acc'] for r in fold_results]\n",
    "\n",
    "print(\"Risultati per fold:\")\n",
    "print(f\"{'Fold':<6} {'Val Acc':<10} {'Test Acc':<10} {'Test Speakers'}\")\n",
    "print(\"-\" * 70)\n",
    "for r in fold_results:\n",
    "    print(f\"{r['fold']+1:<6} {r['best_val_acc']:<10.4f} {r['final_test_acc']:<10.4f} {r['test_speakers']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"MEDIA ± STD:\")\n",
    "print(f\"  Validation Acc: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
    "print(f\"  Test Acc:       {np.mean(test_accs):.4f} ± {np.std(test_accs):.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# %%\n",
    "# STEP 6: VISUALIZZAZIONI\n",
    "\n",
    "# 6A) Plot accuracy per fold\n",
    "rows = 3\n",
    "cols = 3\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(18, 14))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, r in enumerate(fold_results):\n",
    "    ax = axes[idx]\n",
    "    epochs_range = range(1, len(r['val_acc_history']) + 1)\n",
    "    ax.plot(epochs_range, r['val_acc_history'], label='Val', linewidth=2)\n",
    "    ax.plot(epochs_range, r['test_acc_history'], label='Test', linewidth=2)\n",
    "    ax.set_title(f\"Fold {r['fold']+1} (Test Speakers: {r['test_speakers']})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0.75, color='r', linestyle='--', alpha=0.5, label='Target 0.75')\n",
    "\n",
    "# Ultimo subplot: confronto finale\n",
    "ax = axes[-1]\n",
    "x_pos = np.arange(N_FOLDS)\n",
    "ax.bar(x_pos, test_accs, alpha=0.7, label='Test Acc')\n",
    "ax.axhline(y=np.mean(test_accs), color='r', linestyle='--', \n",
    "           label=f'Mean: {np.mean(test_accs):.3f}')\n",
    "ax.axhline(y=0.75, color='g', linestyle='--', label='Target: 0.75')\n",
    "ax.set_xlabel('Fold')\n",
    "ax.set_ylabel('Test Accuracy')\n",
    "ax.set_title('Test Accuracy Comparison')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([f'F{i+1}' for i in range(N_FOLDS)])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6B) Confusion Matrix aggregata (tutti i fold insieme)\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "for r in fold_results:\n",
    "    all_y_true.extend(r['y_true'])\n",
    "    all_y_pred.extend(r['y_pred'])\n",
    "\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix Aggregata (Tutti i Fold)', fontsize=14)\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(8)\n",
    "target_names = [IDX2LABEL[i] for i in range(8)]\n",
    "plt.xticks(tick_marks, target_names, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, target_names)\n",
    "\n",
    "# Aggiungi numeri nelle celle\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             ha=\"center\", va=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report Aggregato (Tutti i Fold):\")\n",
    "print(classification_report(all_y_true, all_y_pred, target_names=target_names, digits=4))\n",
    "\n",
    "# %%\n",
    "# STEP 7: ANALISI SPEAKER-SPECIFIC\n",
    "# Vedi quali speaker sono più difficili da classificare\n",
    "\n",
    "speaker_performance = {}\n",
    "for r in fold_results:\n",
    "    test_spk = r['test_speakers']\n",
    "    test_acc = r['final_test_acc']\n",
    "    for spk in test_spk:\n",
    "        if spk not in speaker_performance:\n",
    "            speaker_performance[spk] = []\n",
    "        speaker_performance[spk].append(test_acc)\n",
    "\n",
    "# Calcola media per speaker (se testato in più fold)\n",
    "speaker_avg = {spk: np.mean(accs) for spk, accs in speaker_performance.items()}\n",
    "speaker_sorted = sorted(speaker_avg.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE PER SPEAKER (quando usato come test set):\")\n",
    "print(\"=\" * 70)\n",
    "for spk, avg_acc in speaker_sorted:\n",
    "    gender = \"F\" if int(spk) % 2 == 0 else \"M\"\n",
    "    print(f\"Speaker {spk} ({gender}): {avg_acc:.4f}\")\n",
    "\n",
    "print(\"\\nNOTA: Speaker con performance molto diverse potrebbero indicare\")\n",
    "print(\"      caratteristiche vocali particolarmente distintive o difficili.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc0ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VISUAL ANALYSIS\n",
    "# t-SNE on embeddings + Grad-CAM su spettrogrammi\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.preprocessing.dataset import IDX2LABEL, parse_ravdess_filename\n",
    "\n",
    "#t-SNE on embeddings \n",
    "@torch.no_grad()\n",
    "def extract_crnn_embeddings(model, loader, device, return_paths=True):\n",
    "    \"\"\"\n",
    "    Estrae embeddings dal CRNN SENZA modificare la classe.\n",
    "    Qui definiamo \"embedding\" come il vettore dopo:\n",
    "    CNN -> BiLSTM -> mean pooling temporale, prima del classifier.\n",
    "    \n",
    "    Ritorna:\n",
    "      - emb: [N, D] numpy\n",
    "      - y_true: [N] numpy\n",
    "      - y_pred: [N] numpy\n",
    "      - paths: lista file (opzionale)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    ys = []\n",
    "    preds = []\n",
    "    paths = []\n",
    "\n",
    "    # Se il tuo Dataset non restituisce il path, lo ricaviamo via loader.dataset.filepaths\n",
    "    # assumendo che loader NON shuffli (true per val/test nel tuo notebook)\n",
    "    dataset_paths = getattr(loader.dataset, \"filepaths\", None)\n",
    "    global_index = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "\n",
    "        # --- ricostruzione \"forward fino all'embedding\" basata sulla tua CRNN ---\n",
    "        feat = model.cnn(x)                      # [B, C, M', T']\n",
    "        b, c, m, t = feat.shape\n",
    "        feat = feat.permute(0, 3, 1, 2).contiguous()  # [B, T', C, M']\n",
    "        feat = feat.view(b, t, c * m)                 # [B, T', C*M']\n",
    "        seq, _ = model.rnn(feat)                      # [B, T', 2H]\n",
    "        emb = seq.mean(dim=1)                         # [B, 2H]  <-- embedding\n",
    "\n",
    "        logits = model.classifier(emb)                # [B, n_classes]\n",
    "        y_hat = logits.argmax(dim=1)\n",
    "\n",
    "        embs.append(emb.detach().cpu())\n",
    "        ys.append(y.detach().cpu())\n",
    "        preds.append(y_hat.detach().cpu())\n",
    "\n",
    "        if return_paths and dataset_paths is not None:\n",
    "            bs = y.size(0)\n",
    "            paths.extend(dataset_paths[global_index: global_index + bs])\n",
    "            global_index += bs\n",
    "\n",
    "    embs = torch.cat(embs, dim=0).numpy()\n",
    "    ys = torch.cat(ys, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "\n",
    "    return embs, ys, preds, paths\n",
    "\n",
    "\n",
    "def tsne_project(embeddings, pca_dim=50, tsne_perplexity=30, tsne_lr=\"auto\", seed=42):\n",
    "    \"\"\"\n",
    "    PCA (opzionale) + t-SNE -> 2D\n",
    "    \"\"\"\n",
    "    X = embeddings\n",
    "    if pca_dim is not None and X.shape[1] > pca_dim:\n",
    "        X = PCA(n_components=pca_dim, random_state=seed).fit_transform(X)\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=tsne_perplexity,\n",
    "        learning_rate=tsne_lr,\n",
    "        init=\"pca\",\n",
    "        random_state=seed\n",
    "    )\n",
    "    Z = tsne.fit_transform(X)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def plot_tsne_by_label(Z, y_true, title=\"t-SNE (colored by true label)\"):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for k in np.unique(y_true):\n",
    "        idx = (y_true == k)\n",
    "        plt.scatter(Z[idx, 0], Z[idx, 1], s=18, alpha=0.75, label=IDX2LABEL[int(k)])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"dim-1\")\n",
    "    plt.ylabel(\"dim-2\")\n",
    "    plt.legend(markerscale=1.2, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_tsne_errors_and_speakers(Z, y_true, y_pred, paths, title=\"t-SNE (errors + speaker id)\"):\n",
    "    \"\"\"\n",
    "    Plot originale: evidenziamo errori e aggiungiamo speaker id.\n",
    "    - corretto: marker 'o'\n",
    "    - errato: marker 'x'\n",
    "    Colore: emotion vera\n",
    "    \"\"\"\n",
    "    correct = (y_true == y_pred)\n",
    "\n",
    "    # Speaker/actor id dal filename (RAVDESS Actor_XX)\n",
    "    speakers = []\n",
    "    for fp in paths:\n",
    "        actor = parse_ravdess_filename(fp)[\"actor\"]\n",
    "        speakers.append(actor)\n",
    "    speakers = np.array(speakers)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Per non avere 24 legende, mettiamo speaker id come testo solo per alcuni punti (campionamento)\n",
    "    # e ci concentriamo su errori vs corretti\n",
    "    for k in np.unique(y_true):\n",
    "        idx_k = (y_true == k)\n",
    "\n",
    "        # corretti\n",
    "        idx_ok = idx_k & correct\n",
    "        plt.scatter(Z[idx_ok, 0], Z[idx_ok, 1], s=16, alpha=0.65, marker=\"o\")\n",
    "\n",
    "        # errati\n",
    "        idx_bad = idx_k & (~correct)\n",
    "        if idx_bad.any():\n",
    "            plt.scatter(Z[idx_bad, 0], Z[idx_bad, 1], s=40, alpha=0.9, marker=\"x\")\n",
    "\n",
    "    # Aggiungi qualche speaker label sui punti sbagliati (molto utile per capire leakage)\n",
    "    bad_idx = np.where(~correct)[0]\n",
    "    for i in bad_idx[:30]:  # limita per non “sporcare” il grafico\n",
    "        plt.text(Z[i, 0], Z[i, 1], speakers[i], fontsize=8, alpha=0.85)\n",
    "\n",
    "    plt.title(title + \"  (x = misclassified; text = speaker id on some errors)\")\n",
    "    plt.xlabel(\"dim-1\")\n",
    "    plt.ylabel(\"dim-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Grad-CAM on spectrograms\n",
    "def find_last_conv2d(module: nn.Module):\n",
    "    \"\"\"\n",
    "    Trova automaticamente l'ultimo nn.Conv2d dentro un modello (o sotto-moduli).\n",
    "    Così se cambi architettura, spesso non devi cambiare nulla.\n",
    "    \"\"\"\n",
    "    last = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last = m\n",
    "    if last is None:\n",
    "        raise RuntimeError(\"Nessun nn.Conv2d trovato: non posso fare Grad-CAM.\")\n",
    "    return last\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM generico:\n",
    "    - hook su ultimo Conv2d (o quello che passi tu)\n",
    "    - produce heatmap (H x W) allineata allo spettrogramma input\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, target_layer: nn.Module):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self.hook_a = target_layer.register_forward_hook(self._forward_hook)\n",
    "        self.hook_g = target_layer.register_full_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, inp, out):\n",
    "        self.activations = out  # [B, C, H, W]\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]  # [B, C, H, W]\n",
    "\n",
    "    def close(self):\n",
    "        self.hook_a.remove()\n",
    "        self.hook_g.remove()\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        import torch.nn as nn\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        # salva stato originale\n",
    "        was_training = self.model.training\n",
    "\n",
    "        # serve per far funzionare backward su cuDNN LSTM\n",
    "        self.model.train()\n",
    "\n",
    "        # disattiva dropout per mappe stabili\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.eval()\n",
    "\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = self.model(x)  # [1, n_classes]\n",
    "        if class_idx is None:\n",
    "            class_idx = int(logits.argmax(dim=1).item())\n",
    "\n",
    "        score = logits[:, class_idx].sum()\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # attivazioni e gradienti dal layer target\n",
    "        A = self.activations          # [1, C, H, W]\n",
    "        G = self.gradients            # [1, C, H, W]\n",
    "        if A is None or G is None:\n",
    "            raise RuntimeError(\"Hooks Grad-CAM non hanno catturato activations/gradients. Controlla target_layer.\")\n",
    "\n",
    "        # pesi: global average pooling dei gradienti\n",
    "        weights = G.mean(dim=(2, 3), keepdim=True)       # [1, C, 1, 1]\n",
    "\n",
    "        # somma pesata delle attivazioni\n",
    "        cam = (weights * A).sum(dim=1, keepdim=True)     # [1, 1, H, W]\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # normalizzazione\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-9)\n",
    "\n",
    "        # upsample alla size dello spettrogramma di input\n",
    "        cam_up = F.interpolate(\n",
    "            cam,\n",
    "            size=(x.shape[2], x.shape[3]),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )  # [1, 1, n_mels, T]\n",
    "\n",
    "        # ripristina stato originale\n",
    "        if not was_training:\n",
    "            self.model.eval()\n",
    "\n",
    "        return cam_up.detach().cpu().squeeze(0).squeeze(0), class_idx, logits.detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "def show_gradcam_on_spectrogram(spec, cam, title=\"\", true_label=None, pred_label=None):\n",
    "    \"\"\"\n",
    "    spec: [1, n_mels, T] tensor (cpu o gpu)\n",
    "    cam:  [n_mels, T] tensor cpu (da GradCAM)\n",
    "    \"\"\"\n",
    "    spec = spec.detach().cpu().squeeze(0)  # [n_mels, T]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.imshow(spec, aspect=\"auto\", origin=\"lower\")\n",
    "    plt.imshow(cam, aspect=\"auto\", origin=\"lower\", alpha=0.45)  # overlay\n",
    "    t = title\n",
    "    if true_label is not None and pred_label is not None:\n",
    "        t += f\" | true={true_label} pred={pred_label}\"\n",
    "    plt.title(t)\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(\"mel bins\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gradcam_demo(model, loader, device, n_examples=10, seed=42, class_mode=\"pred\"):\n",
    "    \"\"\"\n",
    "    Seleziona n_examples campioni casuali e mostra Grad-CAM.\n",
    "    class_mode:\n",
    "      - \"pred\": spiega la classe predetta\n",
    "      - \"true\": spiega la classe vera\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idxs = rng.choice(len(loader.dataset), size=min(n_examples, len(loader.dataset)), replace=False)\n",
    "\n",
    "    target_layer = find_last_conv2d(model)\n",
    "    cam_engine = GradCAM(model, target_layer)\n",
    "\n",
    "    model.eval()\n",
    "    for i in idxs:\n",
    "        spec, y = loader.dataset[i]  # spec: [1, n_mels, T]\n",
    "        x = spec.unsqueeze(0).to(device)  # [1,1,n_mels,T]\n",
    "        y = int(y.item())\n",
    "\n",
    "        if class_mode == \"true\":\n",
    "            cam, cidx, logits = cam_engine(x, class_idx=y)\n",
    "        else:\n",
    "            cam, cidx, logits = cam_engine(x, class_idx=None)\n",
    "\n",
    "        pred = int(logits.argmax(dim=1).item())\n",
    "        show_gradcam_on_spectrogram(\n",
    "            spec,\n",
    "            cam,\n",
    "            title=\"C) Grad-CAM on log-mel spectrogram\",\n",
    "            true_label=IDX2LABEL[y],\n",
    "            pred_label=IDX2LABEL[pred]\n",
    "        )\n",
    "    cam_engine.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac60f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grafico accuracy train vs val\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, val_acc_hist, label=\"Val Accuracy\")\n",
    "plt.plot(epochs_range, test_acc_hist, label=\"Test Accuracy\")\n",
    "plt.title(\"Validation vs Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7) test finale con best checkpoint\n",
    "print(\"\\nBest val acc:\", best_val_acc)\n",
    "model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "print(f\"TEST (best checkpoint) | loss {test_loss:.4f} acc {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "def gradcam_summary(model, loader, device, n_examples=4, seed=42):\n",
    "    \"\"\"\n",
    "    Mostra Grad-CAM su pochi esempi in una figura compatta.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idxs = rng.choice(\n",
    "        len(loader.dataset),\n",
    "        size=min(n_examples, len(loader.dataset)),\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    target_layer = find_last_conv2d(model)\n",
    "    cam_engine = GradCAM(model, target_layer)\n",
    "\n",
    "    fig, axes = plt.subplots(len(idxs), 1, figsize=(12, 3 * len(idxs)))\n",
    "    if len(idxs) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    model.eval()\n",
    "    for ax, i in zip(axes, idxs):\n",
    "        spec, y = loader.dataset[i]\n",
    "        x = spec.unsqueeze(0).to(device)\n",
    "        y = int(y.item())\n",
    "\n",
    "        cam, _, logits = cam_engine(x, class_idx=None)\n",
    "        pred = int(logits.argmax(dim=1).item())\n",
    "\n",
    "        spec_np = spec.squeeze(0).cpu().numpy()\n",
    "        cam_np = cam.numpy()\n",
    "\n",
    "        ax.imshow(spec_np, aspect=\"auto\", origin=\"lower\")\n",
    "        ax.imshow(cam_np, aspect=\"auto\", origin=\"lower\", alpha=0.45)\n",
    "        ax.set_title(\n",
    "            f\"true={IDX2LABEL[y]} | pred={IDX2LABEL[pred]}\",\n",
    "            fontsize=11\n",
    "        )\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"mel bins\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    cam_engine.close()\n",
    "\n",
    "def plot_fold_summary(Z, y_true, y_pred, fold_idx, test_speakers):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # t-SNE – true labels\n",
    "    ax = axes[0]\n",
    "    for k in np.unique(y_true):\n",
    "        idx = (y_true == k)\n",
    "        ax.scatter(\n",
    "            Z[idx, 0], Z[idx, 1],\n",
    "            s=18, alpha=0.7,\n",
    "            label=IDX2LABEL[int(k)]\n",
    "        )\n",
    "    ax.set_title(\"t-SNE (true labels)\")\n",
    "    ax.set_xlabel(\"dim-1\")\n",
    "    ax.set_ylabel(\"dim-2\")\n",
    "    ax.legend(fontsize=8, markerscale=1.1)\n",
    "\n",
    "    # t-SNE – correct vs error\n",
    "    ax = axes[1]\n",
    "    correct = (y_true == y_pred)\n",
    "    ax.scatter(\n",
    "        Z[correct, 0], Z[correct, 1],\n",
    "        s=15, alpha=0.5, label=\"correct\"\n",
    "    )\n",
    "    ax.scatter(\n",
    "        Z[~correct, 0], Z[~correct, 1],\n",
    "        s=40, alpha=0.9, marker=\"x\", label=\"error\"\n",
    "    )\n",
    "    ax.set_title(\"t-SNE (errors)\")\n",
    "    ax.set_xlabel(\"dim-1\")\n",
    "    ax.set_ylabel(\"dim-2\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    ax = axes[2]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "    tick_marks = np.arange(len(IDX2LABEL))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels([IDX2LABEL[i] for i in tick_marks], rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels([IDX2LABEL[i] for i in tick_marks])\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=9\n",
    "            )\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "    # Titolo globale\n",
    "    fig.suptitle(\n",
    "        f\"Fold {fold_idx + 1} | Test speakers: {test_speakers}\",\n",
    "        fontsize=14\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# LOOP SU TUTTI I FOLD\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "splits = list(gkf.split(all_files, labels, groups=actors))\n",
    "\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"VISUAL ANALYSIS – FOLD {fold_idx + 1}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Test loader del fold\n",
    "    _, test_idx = splits[fold_idx]\n",
    "    test_files = [all_files[i] for i in test_idx]\n",
    "    test_speakers = sorted({actors[i] for i in test_idx})\n",
    "\n",
    "    test_ds = RavdessDataset(test_files, augmentation=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Modello del fold\n",
    "    model = CRNN(n_classes=8, n_mels=64).to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"best_fold_{fold_idx}.pt\", map_location=device)\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # Embeddings + t-SNE\n",
    "    emb, y_true_e, y_pred_e, _ = extract_crnn_embeddings(\n",
    "        model, test_loader, device, return_paths=False\n",
    "    )\n",
    "\n",
    "    Z = tsne_project(\n",
    "        emb,\n",
    "        pca_dim=50,\n",
    "        tsne_perplexity=30,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Figura compatta\n",
    "    plot_fold_summary(\n",
    "        Z,\n",
    "        y_true_e,\n",
    "        y_pred_e,\n",
    "        fold_idx,\n",
    "        test_speakers\n",
    "    )\n",
    "\n",
    "    # gradcam\n",
    "\n",
    "    gradcam_summary(\n",
    "        model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        n_examples=4,   # 3–4 è perfetto\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Report testuale \n",
    "    print(\"\\nClassification report:\")\n",
    "    target_names = [IDX2LABEL[i] for i in range(8)]\n",
    "    print(classification_report(\n",
    "        y_true_e, y_pred_e,\n",
    "        target_names=target_names,\n",
    "        digits=4\n",
    "    ))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv310 (3.10.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
