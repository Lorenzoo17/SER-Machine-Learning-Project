{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cd2c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchaudio scikit-learn numpy\n",
    "#!pip install soundfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a0a21f",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b418dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.preprocessing.dataset import (\n",
    "    list_ravdess_files,\n",
    "    filter_audio_speech,\n",
    "    RavdessDataset,\n",
    "    extract_label_idx,\n",
    "    IDX2LABEL,\n",
    ")\n",
    "from src.models.crnn import CRNN\n",
    "from src.utils import set_seed, evaluate\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "from src.visual_analysis.visual_analysis import *\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e13597",
   "metadata": {},
   "source": [
    "# DEFINIZIONE DATA BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59582a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Tot files: 1440\n",
      "Distribuzione classi: Counter({1: 192, 2: 192, 3: 192, 4: 192, 5: 192, 6: 192, 7: 192, 0: 96})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "set_seed(42)\n",
    "\n",
    "DATA_ROOT = \"./data\"\n",
    "EPOCHS = 70\n",
    "BATCH_SIZE = 64\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "N_FOLDS = 6\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# 1) lista file\n",
    "all_files = list_ravdess_files(DATA_ROOT)\n",
    "all_files = filter_audio_speech(all_files)\n",
    "if len(all_files) == 0:\n",
    "    raise RuntimeError(\"Nessun file trovato. Controlla DATA_ROOT e filter_audio_speech().\")\n",
    "\n",
    "# 2) labels per stratify\n",
    "labels = [extract_label_idx(fp) for fp in all_files]\n",
    "print(\"Tot files:\", len(all_files))\n",
    "print(\"Distribuzione classi:\", Counter(labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c4afc0",
   "metadata": {},
   "source": [
    "# SPLIT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f19a8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "GPU name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# serve per vedere se stai usando la CPU o se hai CUDA\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "# IMPORTANTE\n",
    "# se qui la prima riga ti ritorna True allora sotto quando scrivi i DataLoader metti i pinmemory = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3b124db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessing.dataset import parse_ravdess_filename\n",
    "\n",
    "def split_by_speakers(filepaths, train_speakers, val_speakers, test_speakers):\n",
    "    train, val, test = [], [], []\n",
    "    for fp in filepaths:\n",
    "        actor = parse_ravdess_filename(fp)[\"actor\"]\n",
    "        if actor in train_speakers:\n",
    "            train.append(fp)\n",
    "        elif actor in val_speakers:\n",
    "            val.append(fp)\n",
    "        elif actor in test_speakers:\n",
    "            test.append(fp)\n",
    "    return train, val, test\n",
    "\n",
    "\n",
    "# Funzione per Mixup\n",
    "def mixup_data(x, y, alpha=0.2, device='cuda'):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    \n",
    "    batch_size = x.size(0)\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbcf96ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tot files: 1440\n",
      "Distribuzione classi: Counter({1: 192, 2: 192, 3: 192, 4: 192, 5: 192, 6: 192, 7: 192, 0: 96})\n",
      "Numero attori unici: 24\n",
      "\n",
      "============================================================\n",
      "CROSS-VALIDATION LOSO (24 folds)\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "LOSO FOLD 1/24 | Test speaker: 01\n",
      "============================================================\n",
      "Train: 1200 files, 20 speakers ['02', '03', '04', '05', '07', '08', '10', '11', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24']\n",
      "Val:   180 files, 3 speakers ['06', '09', '12']\n",
      "Test:  60 files, 1 speakers ['01']\n",
      "Class counts: Counter({1: 160, 2: 160, 3: 160, 4: 160, 5: 160, 6: 160, 7: 160, 0: 80})\n",
      "Weights: [1.778 0.889 0.889 0.889 0.889 0.889 0.889 0.889]\n",
      "\n",
      "Inizio training fold 1...\n",
      "Epoch   1/70 | train loss 2.0919 acc 0.1233 | val loss 2.0551 acc 0.2278 | test acc 0.1000\n",
      "Epoch   5/70 | train loss 1.8393 acc 0.3075 | val loss 1.7989 acc 0.3333 | test acc 0.2833\n",
      "Epoch  10/70 | train loss 1.5944 acc 0.4633 | val loss 1.6887 acc 0.3667 | test acc 0.4500\n",
      "Epoch  15/70 | train loss 1.4014 acc 0.5633 | val loss 1.3481 acc 0.4944 | test acc 0.4833\n",
      "Epoch  20/70 | train loss 1.3214 acc 0.6000 | val loss 1.3082 acc 0.5833 | test acc 0.5667\n",
      "Epoch  25/70 | train loss 1.1919 acc 0.6900 | val loss 1.3383 acc 0.5556 | test acc 0.5000\n",
      "Epoch  30/70 | train loss 1.0711 acc 0.7425 | val loss 1.0469 acc 0.6500 | test acc 0.6833\n",
      "Epoch  35/70 | train loss 0.9653 acc 0.7925 | val loss 1.0901 acc 0.6500 | test acc 0.6333\n",
      "Epoch  40/70 | train loss 0.9145 acc 0.8167 | val loss 1.1121 acc 0.5944 | test acc 0.6000\n",
      "Epoch  45/70 | train loss 0.8047 acc 0.8883 | val loss 1.1251 acc 0.6444 | test acc 0.6167\n",
      "Epoch  50/70 | train loss 0.7601 acc 0.9008 | val loss 1.1193 acc 0.6778 | test acc 0.5667\n",
      "Epoch  55/70 | train loss 0.7561 acc 0.9067 | val loss 1.1201 acc 0.6556 | test acc 0.5000\n",
      "Epoch  60/70 | train loss 0.7252 acc 0.9208 | val loss 1.1064 acc 0.6556 | test acc 0.5333\n",
      "Epoch  65/70 | train loss 0.6925 acc 0.9408 | val loss 1.1625 acc 0.6556 | test acc 0.6167\n",
      "Epoch  70/70 | train loss 0.6762 acc 0.9492 | val loss 1.0969 acc 0.6556 | test acc 0.5833\n",
      "\n",
      "============================================================\n",
      "RISULTATI FOLD 1\n",
      "============================================================\n",
      "Best Validation Accuracy: 0.6944\n",
      "Final Test Accuracy:      0.6000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     neutral     0.5000    0.2500    0.3333         4\n",
      "        calm     1.0000    0.7500    0.8571         8\n",
      "       happy     0.3478    1.0000    0.5161         8\n",
      "         sad     0.5385    0.8750    0.6667         8\n",
      "       angry     1.0000    0.6250    0.7692         8\n",
      "     fearful     0.5000    0.1250    0.2000         8\n",
      "     disgust     1.0000    0.7500    0.8571         8\n",
      "   surprised     0.6667    0.2500    0.3636         8\n",
      "\n",
      "    accuracy                         0.6000        60\n",
      "   macro avg     0.6941    0.5781    0.5704        60\n",
      "weighted avg     0.7071    0.6000    0.5862        60\n",
      "\n",
      "\n",
      "============================================================\n",
      "LOSO FOLD 2/24 | Test speaker: 02\n",
      "============================================================\n",
      "Train: 1200 files, 20 speakers ['01', '03', '04', '05', '06', '07', '09', '10', '11', '12', '14', '15', '16', '17', '18', '19', '20', '22', '23', '24']\n",
      "Val:   180 files, 3 speakers ['08', '13', '21']\n",
      "Test:  60 files, 1 speakers ['02']\n",
      "Class counts: Counter({1: 160, 2: 160, 3: 160, 4: 160, 5: 160, 6: 160, 7: 160, 0: 80})\n",
      "Weights: [1.778 0.889 0.889 0.889 0.889 0.889 0.889 0.889]\n",
      "\n",
      "Inizio training fold 2...\n",
      "Epoch   1/70 | train loss 2.0805 acc 0.1475 | val loss 2.0323 acc 0.2389 | test acc 0.2833\n",
      "Epoch   5/70 | train loss 1.7905 acc 0.3575 | val loss 1.7666 acc 0.3167 | test acc 0.5333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 171\u001b[0m\n\u001b[0;32m    168\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m total\n\u001b[0;32m    169\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m running_correct \u001b[38;5;241m/\u001b[39m total\n\u001b[1;32m--> 171\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m test_loss_epoch, test_acc_epoch \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, device)\n\u001b[0;32m    174\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(val_acc)\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Universita\\Anno 2025-2026\\Mascia Learning for Vision And Multimedia\\Project\\SER-Machine-Learning-Project\\src\\utils.py:32\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, loader, device)\u001b[0m\n\u001b[0;32m     28\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m     33\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     34\u001b[0m     y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Universita\\Anno 2025-2026\\Mascia Learning for Vision And Multimedia\\Project\\SER-Machine-Learning-Project\\src\\preprocessing\\dataset.py:157\u001b[0m, in \u001b[0;36mRavdessDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    154\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths[idx]\n\u001b[0;32m    155\u001b[0m y \u001b[38;5;241m=\u001b[39m extract_label_idx(path)\n\u001b[1;32m--> 157\u001b[0m wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m    158\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel(wav)          \n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_db \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Universita\\Anno 2025-2026\\Mascia Learning for Vision And Multimedia\\Project\\SER-Machine-Learning-Project\\src\\preprocessing\\dataset.py:126\u001b[0m, in \u001b[0;36mRavdessDataset._load_audio\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_load_audio\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;66;03m# soundfile: ritorna np array shape [N] o [N, C]\u001b[39;00m\n\u001b[1;32m--> 126\u001b[0m     wav_np, sr \u001b[38;5;241m=\u001b[39m \u001b[43msf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [N, C]\u001b[39;00m\n\u001b[0;32m    127\u001b[0m     wav \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(wav_np)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# [C, N]\u001b[39;00m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;66;03m# mono\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\site-packages\\soundfile.py:305\u001b[0m, in \u001b[0;36mread\u001b[1;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mread\u001b[39m(file, frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m, always_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    220\u001b[0m          fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, samplerate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m          \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, subtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, endian\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    222\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Provide audio data from a sound file as NumPy array.\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \n\u001b[0;32m    224\u001b[0m \u001b[38;5;124;03m    By default, the whole file is read from the beginning, but the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    303\u001b[0m \n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mSoundFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplerate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msubtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendian\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    307\u001b[0m         frames \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39m_prepare_read(start, stop, frames)\n\u001b[0;32m    308\u001b[0m         data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread(frames, dtype, always_2d, fill_value, out)\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\site-packages\\soundfile.py:690\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd, compression_level, bitrate_mode)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bitrate_mode \u001b[38;5;241m=\u001b[39m bitrate_mode\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info \u001b[38;5;241m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    689\u001b[0m                                  \u001b[38;5;28mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode_int\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosefd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(mode)\u001b[38;5;241m.\u001b[39missuperset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseekable():\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;66;03m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    693\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\site-packages\\soundfile.py:1242\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1240\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call the appropriate sf_open*() function from libsndfile.\"\"\"\u001b[39;00m\n\u001b[0;32m   1241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file, (_unicode, \u001b[38;5;28mbytes\u001b[39m)):\n\u001b[1;32m-> 1242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_os\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1243\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m   1244\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile exists: \u001b[39m\u001b[38;5;132;01m{0!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname))\n",
      "File \u001b[1;32mc:\\Users\\loren\\anaconda3\\envs\\ml_pytorch\\lib\\genericpath.py:30\u001b[0m, in \u001b[0;36misfile\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path is a regular file\"\"\"\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 30\u001b[0m     st \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "actors = [parse_ravdess_filename(fp)[\"actor\"] for fp in all_files]\n",
    "\n",
    "print(f\"Tot files: {len(all_files)}\")\n",
    "print(f\"Distribuzione classi: {Counter(labels)}\")\n",
    "print(f\"Numero attori unici: {len(set(actors))}\")\n",
    "unique_actors = sorted(set(actors)) # lista speaker unici\n",
    "\n",
    "# Lista per salvare i risultati di ogni fold\n",
    "fold_results = []\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"CROSS-VALIDATION LOSO ({len(unique_actors)} folds)\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "for fold_idx, test_actor  in enumerate(unique_actors):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"LOSO FOLD {fold_idx + 1}/{len(unique_actors)} | Test speaker: {test_actor}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    test_files = [\n",
    "        fp for fp in all_files\n",
    "        if parse_ravdess_filename(fp)[\"actor\"] == test_actor\n",
    "    ]\n",
    "\n",
    "    train_val_files = [\n",
    "        fp for fp in all_files\n",
    "        if parse_ravdess_filename(fp)[\"actor\"] != test_actor\n",
    "    ]\n",
    "\n",
    "    train_val_actors = [\n",
    "        parse_ravdess_filename(fp)[\"actor\"] for fp in train_val_files\n",
    "    ]\n",
    "\n",
    "    test_actors = [test_actor]\n",
    "    \n",
    "    # =====================================================\n",
    "    # B) ULTERIORE SPLIT TRAIN/VAL (speaker-independent anche qui)\n",
    "    # =====================================================\n",
    "    \n",
    "    unique_train_val_actors = list(set(train_val_actors))\n",
    "    # val_size = max(2, len(unique_train_val_actors) // 6)  # ~15-17% per validation\n",
    "    val_size = 3 # fissi per LOSO\n",
    "    \n",
    "    # Bilanciamento per genere (RAVDESS: dispari=M, pari=F)\n",
    "    male_actors_tv = [a for a in unique_train_val_actors if int(a) % 2 == 1]\n",
    "    female_actors_tv = [a for a in unique_train_val_actors if int(a) % 2 == 0]\n",
    "    \n",
    "    # Scegli attori per validation bilanciati per genere\n",
    "    import random\n",
    "    random.seed(42 + fold_idx)  # Seed diverso per ogni fold\n",
    "\n",
    "    n_male = val_size // 2          # 1\n",
    "    n_female = val_size // 2        # 1\n",
    "\n",
    "    # se dispari, assegna l'extra a caso (o al genere più numeroso)\n",
    "    if val_size % 2 == 1:\n",
    "        if len(male_actors_tv) > len(female_actors_tv):\n",
    "            n_male += 1\n",
    "        else:\n",
    "            n_female += 1\n",
    "\n",
    "    val_actors = (\n",
    "        random.sample(male_actors_tv, min(n_male, len(male_actors_tv))) +\n",
    "        random.sample(female_actors_tv, min(n_female, len(female_actors_tv)))\n",
    "    )\n",
    "    train_actors = [a for a in unique_train_val_actors if a not in val_actors]\n",
    "    \n",
    "    # Dividi i file\n",
    "    train_files = [f for f, a in zip(train_val_files, train_val_actors) if a in train_actors]\n",
    "    val_files = [f for f, a in zip(train_val_files, train_val_actors) if a in val_actors]\n",
    "    \n",
    "    print(f\"Train: {len(train_files)} files, {len(train_actors)} speakers {sorted(train_actors)}\")\n",
    "    print(f\"Val:   {len(val_files)} files, {len(val_actors)} speakers {sorted(val_actors)}\")\n",
    "    print(f\"Test:  {len(test_files)} files, {len(test_actors)} speakers {test_actors}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # C) CLASS WEIGHTS\n",
    "    # =====================================================\n",
    "    \n",
    "    train_labels_fold = [extract_label_idx(fp) for fp in train_files]\n",
    "    counts = Counter(train_labels_fold)\n",
    "    \n",
    "    weights = torch.tensor([1.0 / counts[i] for i in range(8)], dtype=torch.float, device=device)\n",
    "    weights = weights / weights.sum() * 8\n",
    "    \n",
    "    print(f\"Class counts: {counts}\")\n",
    "    print(f\"Weights: {weights.detach().cpu().numpy().round(3)}\")\n",
    "    \n",
    "    # =====================================================\n",
    "    # D) DATASET E DATALOADER\n",
    "    # =====================================================\n",
    "    \n",
    "    AUG_ON = True\n",
    "    \n",
    "    aug_cfg = {\n",
    "        \"gain\": True,          # Abilitato\n",
    "        \"gain_db\": (-3, 3),    # Range più ampio\n",
    "        \n",
    "        \"time_shift\": True,    \n",
    "        \"time_shift_s\": 0.03,  # Più generoso\n",
    "        \n",
    "        \"noise\": True,\n",
    "        \"snr_db\": (25, 40),    # SNR più realistico\n",
    "        \n",
    "        \"vtlp\": False,\n",
    "\n",
    "        \"reverb\": False,        # Abilitato\n",
    "        \"reverb_ir_s\": 0.08,\n",
    "        \"reverb_decay\": 0.3,\n",
    "    }\n",
    "    \n",
    "    train_ds = RavdessDataset(train_files, augmentation=AUG_ON, aug_config=aug_cfg)\n",
    "    val_ds   = RavdessDataset(val_files,   augmentation=False)\n",
    "    test_ds  = RavdessDataset(test_files,  augmentation=False)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                              num_workers=0, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=0, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                              num_workers=0, pin_memory=True)\n",
    "    \n",
    "    # =====================================================\n",
    "    # E) MODEL, OPTIMIZER, SCHEDULER\n",
    "    # =====================================================\n",
    "    \n",
    "    model = CRNN(n_classes=8, n_mels=64).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"max\", factor=0.5, patience=6\n",
    "    )\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    best_path = f\"best_fold_{fold_idx}.pt\"  # Path diverso per ogni fold\n",
    "    \n",
    "    # History per questo fold\n",
    "    val_acc_hist = []\n",
    "    test_acc_hist = []\n",
    "    train_acc_hist = []\n",
    "\n",
    "    print(f\"\\nInizio training fold {fold_idx + 1}...\")\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = torch.nn.functional.cross_entropy(\n",
    "                logits, y, weight=weights, label_smoothing=0.1\n",
    "            )\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item() * y.size(0)\n",
    "            running_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "            total += y.size(0)\n",
    "        \n",
    "        train_loss = running_loss / total\n",
    "        train_acc = running_correct / total\n",
    "        \n",
    "        val_loss, val_acc = evaluate(model, val_loader, device)\n",
    "        test_loss_epoch, test_acc_epoch = evaluate(model, test_loader, device)\n",
    "        \n",
    "        scheduler.step(val_acc)\n",
    "        \n",
    "        val_acc_hist.append(val_acc)\n",
    "        test_acc_hist.append(test_acc_epoch)\n",
    "\n",
    "        train_acc_hist.append(train_acc)\n",
    "        \n",
    "        # Print ogni 5 epoche\n",
    "        if epoch % 5 == 0 or epoch == 1:\n",
    "            print(\n",
    "                f\"Epoch {epoch:3d}/{EPOCHS} | \"\n",
    "                f\"train loss {train_loss:.4f} acc {train_acc:.4f} | \"\n",
    "                f\"val loss {val_loss:.4f} acc {val_acc:.4f} | \"\n",
    "                f\"test acc {test_acc_epoch:.4f}\"\n",
    "            )\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "\n",
    "    # =====================================================\n",
    "    # G) VALUTAZIONE FINALE DEL FOLD\n",
    "    # =====================================================\n",
    "    \n",
    "    # Carica il miglior modello\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    \n",
    "    final_test_loss, final_test_acc = evaluate(model, test_loader, device)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"RISULTATI FOLD {fold_idx + 1}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Best Validation Accuracy: {best_val_acc:.4f}\")\n",
    "    print(f\"Final Test Accuracy:      {final_test_acc:.4f}\")\n",
    "    \n",
    "    # Classification report\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            logits = model(x).cpu()\n",
    "            preds = logits.argmax(dim=1).numpy().tolist()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(y.numpy().tolist())\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    target_names = [IDX2LABEL[i] for i in range(8)]\n",
    "    print(classification_report(y_true, y_pred, target_names=target_names, digits=4))\n",
    "    \n",
    "    # =====================================================\n",
    "    # H) SALVA RISULTATI DEL FOLD\n",
    "    # =====================================================\n",
    "    \n",
    "    fold_results.append({\n",
    "        'fold': fold_idx,\n",
    "        'test_speakers': test_actors,\n",
    "        'val_speakers': sorted(val_actors),\n",
    "        'train_speakers': sorted(train_actors),\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'final_test_acc': final_test_acc,\n",
    "        'train_acc_history': train_acc_hist,\n",
    "        'val_acc_history': val_acc_hist,\n",
    "        'test_acc_history': test_acc_hist,\n",
    "        'y_true': y_true,\n",
    "        'y_pred': y_pred,\n",
    "        'confusion_matrix': confusion_matrix(y_true, y_pred),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e62b64e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fold_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# %%\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRISULTATI FINALI LOSO (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[43mfold_results\u001b[49m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Estrai metriche\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'fold_results' is not defined"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"RISULTATI FINALI LOSO ({len(fold_results)})\")\n",
    "print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Estrai metriche\n",
    "test_accs = [r['final_test_acc'] for r in fold_results]\n",
    "val_accs = [r['best_val_acc'] for r in fold_results]\n",
    "\n",
    "# === LOSO: Test Accuracy per Speaker (figura separata) ===\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "x_pos = np.arange(len(fold_results))\n",
    "\n",
    "plt.bar(x_pos, test_accs, alpha=0.7, label='Test Accuracy')\n",
    "plt.axhline(y=np.mean(test_accs), color='r', linestyle='--',\n",
    "            label=f'Mean: {np.mean(test_accs):.3f}')\n",
    "plt.axhline(y=0.75, color='g', linestyle='--', label='Target: 0.75')\n",
    "\n",
    "plt.xlabel('Test Speaker')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('LOSO – Test Accuracy per Speaker')\n",
    "\n",
    "plt.xticks(\n",
    "    x_pos,\n",
    "    [f\"SPK {r['test_speakers'][0]}\" for r in fold_results],\n",
    "    rotation=45\n",
    ")\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Risultati per fold:\")\n",
    "print(f\"{'Fold':<6} {'Val Acc':<10} {'Test Acc':<10} {'Test Speakers'}\")\n",
    "print(\"-\" * 70)\n",
    "for r in fold_results:\n",
    "    print(f\"{r['fold']+1:<6} {r['best_val_acc']:<10.4f} {r['final_test_acc']:<10.4f} {r['test_speakers']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(f\"MEDIA ± STD:\")\n",
    "print(f\"  Validation Acc: {np.mean(val_accs):.4f} ± {np.std(val_accs):.4f}\")\n",
    "print(f\"  Test Acc:       {np.mean(test_accs):.4f} ± {np.std(test_accs):.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# %%\n",
    "# STEP 6: VISUALIZZAZIONI\n",
    "\n",
    "# 6A) Plot accuracy per fold (Dinamico per 24 fold)\n",
    "num_folds = len(fold_results)\n",
    "cols = 4\n",
    "rows = (num_folds + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, r in enumerate(fold_results):\n",
    "    ax = axes[idx]\n",
    "    epochs_range = range(1, len(r['val_acc_history']) + 1)\n",
    "    ax.plot(epochs_range, r['val_acc_history'], label='Val', linewidth=2)\n",
    "    ax.plot(epochs_range, r['test_acc_history'], label='Test', linewidth=2)\n",
    "    ax.set_title(f\"Fold {r['fold']+1} (Test: {r['test_speakers'][0]})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=0.75, color='r', linestyle='--', alpha=0.5)\n",
    "\n",
    "for i in range(num_folds, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 6A-bis) Plot Train vs Validation accuracy per fold (Dinamico per 24 fold)\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(20, 5 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, r in enumerate(fold_results):\n",
    "    ax = axes[idx]\n",
    "    epochs_range = range(1, len(r['train_acc_history']) + 1)\n",
    "\n",
    "    ax.plot(epochs_range, r['train_acc_history'], label='Train', linewidth=2)\n",
    "    ax.plot(epochs_range, r['val_acc_history'], label='Validation', linewidth=2)\n",
    "\n",
    "    ax.set_title(f\"Fold {r['fold']+1} (Test Speaker: {r['test_speakers'][0]})\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Accuracy\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "for i in range(num_folds, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# === LOSO: Curve per speaker (24 grafici separati con chiusura memoria) ===\n",
    "for r in fold_results:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    epochs_range = range(1, len(r['train_acc_history']) + 1)\n",
    "\n",
    "    plt.plot(epochs_range, r['train_acc_history'], label='Train', linewidth=2)\n",
    "    plt.plot(epochs_range, r['val_acc_history'], label='Validation', linewidth=2)\n",
    "    plt.plot(epochs_range, r['test_acc_history'], label='Test', linewidth=2)\n",
    "\n",
    "    plt.title(f\"LOSO – Speaker {r['test_speakers'][0]}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close() \n",
    "\n",
    "# 6B) Confusion Matrix aggregata (tutti i fold insieme)\n",
    "all_y_true = []\n",
    "all_y_pred = []\n",
    "for r in fold_results:\n",
    "    all_y_true.extend(r['y_true'])\n",
    "    all_y_pred.extend(r['y_pred'])\n",
    "\n",
    "cm = confusion_matrix(all_y_true, all_y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix Aggregata (Tutti i Fold)', fontsize=14)\n",
    "plt.colorbar()\n",
    "\n",
    "tick_marks = np.arange(8)\n",
    "target_names = [IDX2LABEL[i] for i in range(8)]\n",
    "plt.xticks(tick_marks, target_names, rotation=45, ha='right')\n",
    "plt.yticks(tick_marks, target_names)\n",
    "\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in np.ndindex(cm.shape):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             ha=\"center\", va=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report Aggregato (Tutti i Fold):\")\n",
    "print(classification_report(all_y_true, all_y_pred, target_names=target_names, digits=4))\n",
    "\n",
    "# %%\n",
    "# STEP 7: ANALISI SPEAKER-SPECIFIC\n",
    "\n",
    "speaker_performance = {}\n",
    "for r in fold_results:\n",
    "    test_spk = r['test_speakers']\n",
    "    test_acc = r['final_test_acc']\n",
    "    for spk in test_spk:\n",
    "        if spk not in speaker_performance:\n",
    "            speaker_performance[spk] = []\n",
    "        speaker_performance[spk].append(test_acc)\n",
    "\n",
    "speaker_avg = {spk: np.mean(accs) for spk, accs in speaker_performance.items()}\n",
    "speaker_sorted = sorted(speaker_avg.items(), key=lambda x: x[1])\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE PER SPEAKER (quando usato come test set):\")\n",
    "print(\"=\" * 70)\n",
    "for spk, avg_acc in speaker_sorted:\n",
    "    gender = \"F\" if int(spk) % 2 == 0 else \"M\"\n",
    "    print(f\"Speaker {spk} ({gender}): {avg_acc:.4f}\")\n",
    "\n",
    "# Analisi per genere\n",
    "male_accs = [acc for spk, acc in speaker_avg.items() if int(spk) % 2 != 0]\n",
    "female_accs = [acc for spk, acc in speaker_avg.items() if int(spk) % 2 == 0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PERFORMANCE MEDIA PER GENERE:\")\n",
    "print(\"-\" * 70)\n",
    "if male_accs: print(f\"Media Maschi:  {np.mean(male_accs):.4f} ± {np.std(male_accs):.4f}\")\n",
    "if female_accs: print(f\"Media Femmine: {np.mean(female_accs):.4f} ± {np.std(female_accs):.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nNOTA: Speaker con performance molto diverse potrebbero indicare\")\n",
    "print(\"      caratteristiche vocali particolarmente distintive o difficili.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbc0ecfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# VISUAL ANALYSIS\n",
    "# t-SNE on embeddings + Grad-CAM su spettrogrammi\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from src.preprocessing.dataset import IDX2LABEL, parse_ravdess_filename\n",
    "\n",
    "#t-SNE on embeddings \n",
    "@torch.no_grad()\n",
    "def extract_crnn_embeddings(model, loader, device, return_paths=True):\n",
    "    \"\"\"\n",
    "    Estrae embeddings dal CRNN SENZA modificare la classe.\n",
    "    Qui definiamo \"embedding\" come il vettore dopo:\n",
    "    CNN -> BiLSTM -> mean pooling temporale, prima del classifier.\n",
    "    \n",
    "    Ritorna:\n",
    "      - emb: [N, D] numpy\n",
    "      - y_true: [N] numpy\n",
    "      - y_pred: [N] numpy\n",
    "      - paths: lista file (opzionale)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    embs = []\n",
    "    ys = []\n",
    "    preds = []\n",
    "    paths = []\n",
    "\n",
    "    # Se il tuo Dataset non restituisce il path, lo ricaviamo via loader.dataset.filepaths\n",
    "    # assumendo che loader NON shuffli (true per val/test nel tuo notebook)\n",
    "    dataset_paths = getattr(loader.dataset, \"filepaths\", None)\n",
    "    global_index = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x, y = batch\n",
    "        x = x.to(device)\n",
    "\n",
    "        # --- ricostruzione \"forward fino all'embedding\" basata sulla tua CRNN ---\n",
    "        feat = model.cnn(x)                      # [B, C, M', T']\n",
    "        b, c, m, t = feat.shape\n",
    "        feat = feat.permute(0, 3, 1, 2).contiguous()  # [B, T', C, M']\n",
    "        feat = feat.view(b, t, c * m)                 # [B, T', C*M']\n",
    "        seq, _ = model.rnn(feat)                      # [B, T', 2H]\n",
    "        emb = seq.mean(dim=1)                         # [B, 2H]  <-- embedding\n",
    "\n",
    "        logits = model.classifier(emb)                # [B, n_classes]\n",
    "        y_hat = logits.argmax(dim=1)\n",
    "\n",
    "        embs.append(emb.detach().cpu())\n",
    "        ys.append(y.detach().cpu())\n",
    "        preds.append(y_hat.detach().cpu())\n",
    "\n",
    "        if return_paths and dataset_paths is not None:\n",
    "            bs = y.size(0)\n",
    "            paths.extend(dataset_paths[global_index: global_index + bs])\n",
    "            global_index += bs\n",
    "\n",
    "    embs = torch.cat(embs, dim=0).numpy()\n",
    "    ys = torch.cat(ys, dim=0).numpy()\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "\n",
    "    return embs, ys, preds, paths\n",
    "\n",
    "\n",
    "def tsne_project(embeddings, pca_dim=50, tsne_perplexity=30, tsne_lr=\"auto\", seed=42):\n",
    "    \"\"\"\n",
    "    PCA (opzionale) + t-SNE -> 2D\n",
    "    \"\"\"\n",
    "    X = embeddings\n",
    "    if pca_dim is not None and X.shape[1] > pca_dim:\n",
    "        X = PCA(n_components=pca_dim, random_state=seed).fit_transform(X)\n",
    "\n",
    "    tsne = TSNE(\n",
    "        n_components=2,\n",
    "        perplexity=tsne_perplexity,\n",
    "        learning_rate=tsne_lr,\n",
    "        init=\"pca\",\n",
    "        random_state=seed\n",
    "    )\n",
    "    Z = tsne.fit_transform(X)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def plot_tsne_by_label(Z, y_true, title=\"t-SNE (colored by true label)\"):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    for k in np.unique(y_true):\n",
    "        idx = (y_true == k)\n",
    "        plt.scatter(Z[idx, 0], Z[idx, 1], s=18, alpha=0.75, label=IDX2LABEL[int(k)])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"dim-1\")\n",
    "    plt.ylabel(\"dim-2\")\n",
    "    plt.legend(markerscale=1.2, bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_tsne_errors_and_speakers(Z, y_true, y_pred, paths, title=\"t-SNE (errors + speaker id)\"):\n",
    "    \"\"\"\n",
    "    Plot originale: evidenziamo errori e aggiungiamo speaker id.\n",
    "    - corretto: marker 'o'\n",
    "    - errato: marker 'x'\n",
    "    Colore: emotion vera\n",
    "    \"\"\"\n",
    "    correct = (y_true == y_pred)\n",
    "\n",
    "    # Speaker/actor id dal filename (RAVDESS Actor_XX)\n",
    "    speakers = []\n",
    "    for fp in paths:\n",
    "        actor = parse_ravdess_filename(fp)[\"actor\"]\n",
    "        speakers.append(actor)\n",
    "    speakers = np.array(speakers)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "\n",
    "    # Per non avere 24 legende, mettiamo speaker id come testo solo per alcuni punti (campionamento)\n",
    "    # e ci concentriamo su errori vs corretti\n",
    "    for k in np.unique(y_true):\n",
    "        idx_k = (y_true == k)\n",
    "\n",
    "        # corretti\n",
    "        idx_ok = idx_k & correct\n",
    "        plt.scatter(Z[idx_ok, 0], Z[idx_ok, 1], s=16, alpha=0.65, marker=\"o\")\n",
    "\n",
    "        # errati\n",
    "        idx_bad = idx_k & (~correct)\n",
    "        if idx_bad.any():\n",
    "            plt.scatter(Z[idx_bad, 0], Z[idx_bad, 1], s=40, alpha=0.9, marker=\"x\")\n",
    "\n",
    "    # Aggiungi qualche speaker label sui punti sbagliati (molto utile per capire leakage)\n",
    "    bad_idx = np.where(~correct)[0]\n",
    "    for i in bad_idx[:30]:  # limita per non “sporcare” il grafico\n",
    "        plt.text(Z[i, 0], Z[i, 1], speakers[i], fontsize=8, alpha=0.85)\n",
    "\n",
    "    plt.title(title + \"  (x = misclassified; text = speaker id on some errors)\")\n",
    "    plt.xlabel(\"dim-1\")\n",
    "    plt.ylabel(\"dim-2\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#Grad-CAM on spectrograms\n",
    "def find_last_conv2d(module: nn.Module):\n",
    "    \"\"\"\n",
    "    Trova automaticamente l'ultimo nn.Conv2d dentro un modello (o sotto-moduli).\n",
    "    Così se cambi architettura, spesso non devi cambiare nulla.\n",
    "    \"\"\"\n",
    "    last = None\n",
    "    for m in module.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            last = m\n",
    "    if last is None:\n",
    "        raise RuntimeError(\"Nessun nn.Conv2d trovato: non posso fare Grad-CAM.\")\n",
    "    return last\n",
    "\n",
    "\n",
    "class GradCAM:\n",
    "    \"\"\"\n",
    "    Grad-CAM generico:\n",
    "    - hook su ultimo Conv2d (o quello che passi tu)\n",
    "    - produce heatmap (H x W) allineata allo spettrogramma input\n",
    "    \"\"\"\n",
    "    def __init__(self, model: nn.Module, target_layer: nn.Module):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.activations = None\n",
    "        self.gradients = None\n",
    "        self.hook_a = target_layer.register_forward_hook(self._forward_hook)\n",
    "        self.hook_g = target_layer.register_full_backward_hook(self._backward_hook)\n",
    "\n",
    "    def _forward_hook(self, module, inp, out):\n",
    "        self.activations = out  # [B, C, H, W]\n",
    "\n",
    "    def _backward_hook(self, module, grad_input, grad_output):\n",
    "        self.gradients = grad_output[0]  # [B, C, H, W]\n",
    "\n",
    "    def close(self):\n",
    "        self.hook_a.remove()\n",
    "        self.hook_g.remove()\n",
    "\n",
    "    def __call__(self, x, class_idx=None):\n",
    "        import torch.nn as nn\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        # salva stato originale\n",
    "        was_training = self.model.training\n",
    "\n",
    "        # serve per far funzionare backward su cuDNN LSTM\n",
    "        self.model.train()\n",
    "\n",
    "        # disattiva dropout per mappe stabili\n",
    "        for m in self.model.modules():\n",
    "            if isinstance(m, nn.Dropout):\n",
    "                m.eval()\n",
    "\n",
    "        self.model.zero_grad(set_to_none=True)\n",
    "\n",
    "        logits = self.model(x)  # [1, n_classes]\n",
    "        if class_idx is None:\n",
    "            class_idx = int(logits.argmax(dim=1).item())\n",
    "\n",
    "        score = logits[:, class_idx].sum()\n",
    "        score.backward(retain_graph=True)\n",
    "\n",
    "        # attivazioni e gradienti dal layer target\n",
    "        A = self.activations          # [1, C, H, W]\n",
    "        G = self.gradients            # [1, C, H, W]\n",
    "        if A is None or G is None:\n",
    "            raise RuntimeError(\"Hooks Grad-CAM non hanno catturato activations/gradients. Controlla target_layer.\")\n",
    "\n",
    "        # pesi: global average pooling dei gradienti\n",
    "        weights = G.mean(dim=(2, 3), keepdim=True)       # [1, C, 1, 1]\n",
    "\n",
    "        # somma pesata delle attivazioni\n",
    "        cam = (weights * A).sum(dim=1, keepdim=True)     # [1, 1, H, W]\n",
    "        cam = F.relu(cam)\n",
    "\n",
    "        # normalizzazione\n",
    "        cam = cam - cam.min()\n",
    "        cam = cam / (cam.max() + 1e-9)\n",
    "\n",
    "        # upsample alla size dello spettrogramma di input\n",
    "        cam_up = F.interpolate(\n",
    "            cam,\n",
    "            size=(x.shape[2], x.shape[3]),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False\n",
    "        )  # [1, 1, n_mels, T]\n",
    "\n",
    "        # ripristina stato originale\n",
    "        if not was_training:\n",
    "            self.model.eval()\n",
    "\n",
    "        return cam_up.detach().cpu().squeeze(0).squeeze(0), class_idx, logits.detach().cpu()\n",
    "\n",
    "\n",
    "\n",
    "def show_gradcam_on_spectrogram(spec, cam, title=\"\", true_label=None, pred_label=None):\n",
    "    \"\"\"\n",
    "    spec: [1, n_mels, T] tensor (cpu o gpu)\n",
    "    cam:  [n_mels, T] tensor cpu (da GradCAM)\n",
    "    \"\"\"\n",
    "    spec = spec.detach().cpu().squeeze(0)  # [n_mels, T]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.imshow(spec, aspect=\"auto\", origin=\"lower\")\n",
    "    plt.imshow(cam, aspect=\"auto\", origin=\"lower\", alpha=0.45)  # overlay\n",
    "    t = title\n",
    "    if true_label is not None and pred_label is not None:\n",
    "        t += f\" | true={true_label} pred={pred_label}\"\n",
    "    plt.title(t)\n",
    "    plt.xlabel(\"time\")\n",
    "    plt.ylabel(\"mel bins\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def gradcam_demo(model, loader, device, n_examples=10, seed=42, class_mode=\"pred\"):\n",
    "    \"\"\"\n",
    "    Seleziona n_examples campioni casuali e mostra Grad-CAM.\n",
    "    class_mode:\n",
    "      - \"pred\": spiega la classe predetta\n",
    "      - \"true\": spiega la classe vera\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idxs = rng.choice(len(loader.dataset), size=min(n_examples, len(loader.dataset)), replace=False)\n",
    "\n",
    "    target_layer = find_last_conv2d(model)\n",
    "    cam_engine = GradCAM(model, target_layer)\n",
    "\n",
    "    model.eval()\n",
    "    for i in idxs:\n",
    "        spec, y = loader.dataset[i]  # spec: [1, n_mels, T]\n",
    "        x = spec.unsqueeze(0).to(device)  # [1,1,n_mels,T]\n",
    "        y = int(y.item())\n",
    "\n",
    "        if class_mode == \"true\":\n",
    "            cam, cidx, logits = cam_engine(x, class_idx=y)\n",
    "        else:\n",
    "            cam, cidx, logits = cam_engine(x, class_idx=None)\n",
    "\n",
    "        pred = int(logits.argmax(dim=1).item())\n",
    "        show_gradcam_on_spectrogram(\n",
    "            spec,\n",
    "            cam,\n",
    "            title=\"C) Grad-CAM on log-mel spectrogram\",\n",
    "            true_label=IDX2LABEL[y],\n",
    "            pred_label=IDX2LABEL[pred]\n",
    "        )\n",
    "    cam_engine.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac60f8f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EPOCHS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Grafico accuracy train vs val\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m epochs_range \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[43mEPOCHS\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs_range, val_acc_hist, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVal Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EPOCHS' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Grafico accuracy train vs val\n",
    "epochs_range = range(1, EPOCHS + 1)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, val_acc_hist, label=\"Val Accuracy\")\n",
    "plt.plot(epochs_range, test_acc_hist, label=\"Test Accuracy\")\n",
    "plt.title(\"Validation vs Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7) test finale con best checkpoint\n",
    "print(\"\\nBest val acc:\", best_val_acc)\n",
    "model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "\n",
    "test_loss, test_acc = evaluate(model, test_loader, device)\n",
    "print(f\"TEST (best checkpoint) | loss {test_loss:.4f} acc {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "def gradcam_summary(model, loader, device, n_examples=4, seed=42):\n",
    "    \"\"\"\n",
    "    Mostra Grad-CAM su pochi esempi in una figura compatta.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(seed)\n",
    "    idxs = rng.choice(\n",
    "        len(loader.dataset),\n",
    "        size=min(n_examples, len(loader.dataset)),\n",
    "        replace=False\n",
    "    )\n",
    "\n",
    "    target_layer = find_last_conv2d(model)\n",
    "    cam_engine = GradCAM(model, target_layer)\n",
    "\n",
    "    fig, axes = plt.subplots(len(idxs), 1, figsize=(12, 3 * len(idxs)))\n",
    "    if len(idxs) == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    model.eval()\n",
    "    for ax, i in zip(axes, idxs):\n",
    "        spec, y = loader.dataset[i]\n",
    "        x = spec.unsqueeze(0).to(device)\n",
    "        y = int(y.item())\n",
    "\n",
    "        cam, _, logits = cam_engine(x, class_idx=None)\n",
    "        pred = int(logits.argmax(dim=1).item())\n",
    "\n",
    "        spec_np = spec.squeeze(0).cpu().numpy()\n",
    "        cam_np = cam.numpy()\n",
    "\n",
    "        ax.imshow(spec_np, aspect=\"auto\", origin=\"lower\")\n",
    "        ax.imshow(cam_np, aspect=\"auto\", origin=\"lower\", alpha=0.45)\n",
    "        ax.set_title(\n",
    "            f\"true={IDX2LABEL[y]} | pred={IDX2LABEL[pred]}\",\n",
    "            fontsize=11\n",
    "        )\n",
    "        ax.set_xlabel(\"time\")\n",
    "        ax.set_ylabel(\"mel bins\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    cam_engine.close()\n",
    "\n",
    "def plot_fold_summary(Z, y_true, y_pred, fold_idx, test_speakers):\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "    # t-SNE – true labels\n",
    "    ax = axes[0]\n",
    "    for k in np.unique(y_true):\n",
    "        idx = (y_true == k)\n",
    "        ax.scatter(\n",
    "            Z[idx, 0], Z[idx, 1],\n",
    "            s=18, alpha=0.7,\n",
    "            label=IDX2LABEL[int(k)]\n",
    "        )\n",
    "    ax.set_title(\"t-SNE (true labels)\")\n",
    "    ax.set_xlabel(\"dim-1\")\n",
    "    ax.set_ylabel(\"dim-2\")\n",
    "    ax.legend(fontsize=8, markerscale=1.1)\n",
    "\n",
    "    # t-SNE – correct vs error\n",
    "    ax = axes[1]\n",
    "    correct = (y_true == y_pred)\n",
    "    ax.scatter(\n",
    "        Z[correct, 0], Z[correct, 1],\n",
    "        s=15, alpha=0.5, label=\"correct\"\n",
    "    )\n",
    "    ax.scatter(\n",
    "        Z[~correct, 0], Z[~correct, 1],\n",
    "        s=40, alpha=0.9, marker=\"x\", label=\"error\"\n",
    "    )\n",
    "    ax.set_title(\"t-SNE (errors)\")\n",
    "    ax.set_xlabel(\"dim-1\")\n",
    "    ax.set_ylabel(\"dim-2\")\n",
    "    ax.legend()\n",
    "\n",
    "    # Confusion Matrix\n",
    "    ax = axes[2]\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_title(\"Confusion Matrix\")\n",
    "\n",
    "    tick_marks = np.arange(len(IDX2LABEL))\n",
    "    ax.set_xticks(tick_marks)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_xticklabels([IDX2LABEL[i] for i in tick_marks], rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels([IDX2LABEL[i] for i in tick_marks])\n",
    "\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(\n",
    "                j, i, cm[i, j],\n",
    "                ha=\"center\", va=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "                fontsize=9\n",
    "            )\n",
    "\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046)\n",
    "\n",
    "    # Titolo globale\n",
    "    fig.suptitle(\n",
    "        f\"Fold {fold_idx + 1} | Test speakers: {test_speakers}\",\n",
    "        fontsize=14\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# LOOP SU TUTTI I FOLD\n",
    "gkf = GroupKFold(n_splits=N_FOLDS)\n",
    "splits = list(gkf.split(all_files, labels, groups=actors))\n",
    "\n",
    "for fold_idx in range(N_FOLDS):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"VISUAL ANALYSIS – FOLD {fold_idx + 1}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Test loader del fold\n",
    "    _, test_idx = splits[fold_idx]\n",
    "    test_files = [all_files[i] for i in test_idx]\n",
    "    test_speakers = sorted({actors[i] for i in test_idx})\n",
    "\n",
    "    test_ds = RavdessDataset(test_files, augmentation=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_ds,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    # Modello del fold\n",
    "    model = CRNN(n_classes=8, n_mels=64).to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(f\"best_fold_{fold_idx}.pt\", map_location=device)\n",
    "    )\n",
    "    model.eval()\n",
    "\n",
    "    # Embeddings + t-SNE\n",
    "    emb, y_true_e, y_pred_e, _ = extract_crnn_embeddings(\n",
    "        model, test_loader, device, return_paths=False\n",
    "    )\n",
    "\n",
    "    Z = tsne_project(\n",
    "        emb,\n",
    "        pca_dim=50,\n",
    "        tsne_perplexity=30,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Figura compatta\n",
    "    plot_fold_summary(\n",
    "        Z,\n",
    "        y_true_e,\n",
    "        y_pred_e,\n",
    "        fold_idx,\n",
    "        test_speakers\n",
    "    )\n",
    "\n",
    "    # gradcam\n",
    "\n",
    "    gradcam_summary(\n",
    "        model,\n",
    "        test_loader,\n",
    "        device,\n",
    "        n_examples=4,   # 3–4 è perfetto\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Report testuale \n",
    "    print(\"\\nClassification report:\")\n",
    "    target_names = [IDX2LABEL[i] for i in range(8)]\n",
    "    print(classification_report(\n",
    "        y_true_e, y_pred_e,\n",
    "        target_names=target_names,\n",
    "        digits=4\n",
    "    ))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
